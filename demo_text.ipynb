{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test preprocessing code adopted from https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "test process and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from Model import Transformer\n",
    "\n",
    "from torch import  optim\n",
    "import numpy as np\n",
    "import copy\n",
    "import argparse\n",
    "from Mask import create_masks\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_words = 3 # Count SOS and EOS\n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = s.lower() #unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?,'])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?,ÄäÖöÜüẞß']+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    pairs = []\n",
    "    line1 = open('data/train.de', encoding='utf-8').read().strip().split('\\n')\n",
    "    line2 = open('data/train.en', encoding='utf-8').read().strip().split('\\n')  #.splitlines()\n",
    "    \n",
    "    for i in range(len(line1)):\n",
    "        \n",
    "        pairs.append([normalize_string(line1[i]), normalize_string(line2[i])])\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 196884 sentence pairs\n",
      "Indexing words...\n",
      "['wenn sie sich die daten auf dem bildschirm oben anschauen also herzfrequenz , puls , sauerstoff , atemfrequenz sind sie alle ungewöhnlich für ein normales kind , aber normal für dieses kind .', \"now , when you look at some of the data on the screen above , things like heart rate , pulse , oxygen , respiration rates , they 're all unusual for a normal child , but they 're quite normal for the child there ,\"]\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('ger', 'en')\n",
    "\n",
    "# Print an example pair\n",
    "print(random.choice(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['das meiste ist unerforscht , und doch gibt es schönheiten wie diese , die uns fesseln und uns vertrauter mit ihm machen .', \"it 's mostly unexplored , and yet there are beautiful sights like this that captivate us and make us become familiar with it .\"]\n"
     ]
    }
   ],
   "source": [
    "print(pairs[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def variables_from_pair(pair, input_lang, output_lang):\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_len(pair):\n",
    "    result = 0\n",
    "    \n",
    "    \n",
    "    for sents in pair:\n",
    "        for item in sents:\n",
    "            if len(item.split()) > result:\n",
    "                \n",
    "                result = len(item.split())\n",
    "    return result\n",
    "\n",
    "def find_len(element):\n",
    "    result = 0\n",
    "    for item in element:\n",
    "        if len(item.split()) > result:\n",
    "            result = len(item.split())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paddingSOS(vector, max_len):\n",
    "    vector = [SOS_token]+vector\n",
    "    while len(vector)< max_len:\n",
    "        vector.append(PAD_token)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paddingEOS(vector, max_len):\n",
    "    vector = vector + [EOS_token]\n",
    "    while len(vector)< max_len:\n",
    "        vector.append(PAD_token)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_both(vector, max_len):\n",
    "    vector = [SOS_token]+ vector + [EOS_token]\n",
    "    while len(vector)< max_len:\n",
    "        vector.append(PAD_token)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(vector, max_len):\n",
    "    \n",
    "    while len(vector)< max_len:\n",
    "        vector.append(PAD_token)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_lang 0:  PAD\n",
      "input_lang 1:  SOS\n",
      "input_lang 2:  EOS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('input_lang 0: ', input_lang.index2word[0])\n",
    "print('input_lang 1: ', input_lang.index2word[1])\n",
    "print('input_lang 2: ', input_lang.index2word[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len:  623\n"
     ]
    }
   ],
   "source": [
    "max_len = find_max_len(pairs)+2\n",
    "print('max_len: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  ger\n",
      "output:  en\n"
     ]
    }
   ],
   "source": [
    "print('input: ', input_lang.name)\n",
    "print('output: ', output_lang.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data store pair sentences converted to indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('convert sentence to indexes......')\n",
    "# data = pair_to_indexes(pairs, max_len, input_lang, output_lang)\n",
    "\n",
    "# print('all sentence convert finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Text import indexes_from_sentence\n",
    "from Text import padding_both\n",
    "\n",
    "from Mask import create_masks\n",
    "\n",
    "def translate_sentence(sent, model, input_lang, output_lang, maxlen):\n",
    "    sent_as_index = indexes_from_sentence(input_lang, sent)\n",
    "    input = padding_both(sent_as_index, maxlen)\n",
    "    input = torch.Tensor(input)\n",
    "    input = input.cuda()\n",
    "    source = input.unsqueeze(0) # add a dimension\n",
    "\n",
    "    target = torch.zeros((1, maxlen))\n",
    "    target[0][0] = 1\n",
    "    target = target.cuda()\n",
    "\n",
    "    source_mask, target_mask = create_masks(source.cpu(), target.cpu())\n",
    "    source_mask = source_mask.cuda()\n",
    "    target_mask = target_mask.cuda()\n",
    "    output = model(source, target, source_mask, target_mask)\n",
    "\n",
    "    output = F.softmax(output, -1)\n",
    "    out = torch.max(output, -1)[1]  # 1 is index, 0 is max malue\n",
    "    out = out.squeeze(0)\n",
    "    print('out: ', out)\n",
    "    result = ''\n",
    "    for idx in out:\n",
    "        if idx == 0:\n",
    "            break\n",
    "        index = idx.item()\n",
    "        result += output_lang.index2word[index]+' '\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_to_indexes(pairs, max_len, input_lang, output_lang):\n",
    "    source = np.zeros((len(pairs), max_len))\n",
    "    target = np.zeros((len(pairs), max_len))\n",
    "    for i in range(len(pairs)):\n",
    "        # add start token for english\n",
    "        sent2 = padding_both(indexes_from_sentence(output_lang, pairs[i][1]), max_len)\n",
    "        sent2 = torch.Tensor(sent2)\n",
    "        target[i] = sent2\n",
    "        \n",
    "        # add end token for german\n",
    "        sent1 = padding(indexes_from_sentence(input_lang, pairs[i][0]), max_len)\n",
    "        sent1 = torch.Tensor(sent1)\n",
    "        source[i] = sent1\n",
    "    \n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: 1. add bleu score for each epoch 2. change datas to pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "epoch: 0, loss: 22.04, time: 0.74 sec\n",
      "epoch: 1, loss: 16.56, time: 1.01 sec\n",
      "epoch: 2, loss: 12.44, time: 0.60 sec\n",
      "epoch: 3, loss: 9.28, time: 0.48 sec\n",
      "epoch: 4, loss: 6.74, time: 0.42 sec\n",
      "epoch: 5, loss: 4.70, time: 0.45 sec\n",
      "epoch: 6, loss: 3.33, time: 0.52 sec\n",
      "epoch: 7, loss: 2.29, time: 0.47 sec\n",
      "epoch: 8, loss: 1.55, time: 0.56 sec\n",
      "epoch: 9, loss: 1.09, time: 0.58 sec\n",
      "epoch: 10, loss: 0.78, time: 0.56 sec\n",
      "epoch: 11, loss: 0.45, time: 0.56 sec\n",
      "epoch: 12, loss: 0.28, time: 0.67 sec\n",
      "epoch: 13, loss: 0.19, time: 0.42 sec\n",
      "epoch: 14, loss: 0.09, time: 0.50 sec\n"
     ]
    }
   ],
   "source": [
    "save_each = 1\n",
    "#file = open(“loss.txt”,”w”) \n",
    "\n",
    "\n",
    "# dataset: pairs\n",
    "def train_lm(sources, targets, params, net):\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "    sources = torch.from_numpy(sources)\n",
    "    targets = torch.from_numpy(targets)\n",
    "    sources = sources.cuda()\n",
    "    targets = targets.cuda()\n",
    "    \n",
    "    num_examples = sources.size(0)\n",
    "    batches = [(start, start + params['batch_size']) for start in \\\n",
    "               range(0, num_examples, params['batch_size'])]\n",
    "\n",
    "    for epoch in range(params['epochs']):\n",
    "        ep_loss = 0.\n",
    "        start_time = time.time()\n",
    "        random.shuffle(batches)\n",
    "        \n",
    "        \n",
    "        # for each batch, calculate loss and optimize model parameters\n",
    "        for b_idx, (start, end) in enumerate(batches):\n",
    "            source = sources[start:end]\n",
    "            target = targets[start:end]\n",
    "            \n",
    "            \n",
    "            source_mask, target_mask = create_masks(source.cpu(), target.cpu())\n",
    "            source_mask = source_mask.cuda()\n",
    "            target_mask = target_mask.cuda()\n",
    "            preds = net(source, target, source_mask, target_mask)\n",
    "\n",
    "            \n",
    "            \n",
    "            preds = preds.contiguous().view(-1, net.target_vocab)\n",
    "               \n",
    "            labels = target.contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(preds, labels.long())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            ep_loss += loss.item()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        curr_loss = 'epoch: %d, loss: %0.2f, time: %0.2f sec' % (epoch, ep_loss, time.time() - start_time)\n",
    "        print(curr_loss)\n",
    "        #file.write(curr_loss) \n",
    "        # save model each .. epoch\n",
    "        \n",
    "        if epoch % save_each == 0:\n",
    "            substr = 'mytraining'+str(epoch)+'.pt'\n",
    "            path = 'models/'  +substr\n",
    "            torch.save(net.state_dict(), path)\n",
    "        \n",
    "            \n",
    "#file.close()\n",
    "params = {}\n",
    "\n",
    "\n",
    "params['batch_size'] = 16\n",
    "params['epochs'] = 15\n",
    "params['learning_rate'] = 0.001\n",
    "\n",
    "\n",
    "dim_model = 128\n",
    "H = 2\n",
    "N = 3\n",
    "src_vocab = input_lang.n_words\n",
    "trg_vocab = output_lang.n_words\n",
    "\n",
    "model = Transformer(src_vocab, trg_vocab, dim_model, N, H)\n",
    "model = model.cuda()\n",
    "print(10)\n",
    "data_1 = [element for element in pairs if find_len(element) < 100]\n",
    "data_1 = data_1[:20]\n",
    "max_len_1 = find_max_len(data_1) + 2\n",
    "source, target = pair_to_indexes(data_1, max_len_1, input_lang, output_lang)\n",
    "train_lm(source, target, params, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## restore model from .pt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the_model = TheModelClass(*args, **kwargs)\n",
    "# the_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path1 = 'models/mytraining2.pt'\n",
    "# model1 = Transformer(src_vocab, trg_vocab, dim_model, N, H)\n",
    "# model1.load_state_dict(torch.load(path1))\n",
    "\n",
    "\n",
    "# source, target = pair_to_indexes(datas, max_len, input_lang, output_lang)\n",
    "\n",
    "# train_lm(source, target, params, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['david gallo das ist bill lange . ich bin dave gallo .', \"david gallo this is bill lange . i 'm dave gallo .\"]\n",
      "1\n",
      "2\n",
      "out:  tensor([  1,  70,  70,  70,  70,  70,  70,  70,  70, 150, 150, 150, 150, 150,\n",
      "         70,  70, 150, 150, 150, 150], device='cuda:0')\n",
      "SOS depth depth depth depth depth depth depth depth so so so so so depth depth so so so so \n",
      "SOS depth depth depth depth depth depth depth depth so so so so so depth depth so so so so \n"
     ]
    }
   ],
   "source": [
    "print(data_1[0])\n",
    "dim_model = 128\n",
    "H = 2\n",
    "N = 3\n",
    "src_vocab = input_lang.n_words\n",
    "trg_vocab = output_lang.n_words\n",
    "\n",
    "print(1)\n",
    "t_model = Transformer(src_vocab, trg_vocab, dim_model, N, H)\n",
    "t_model.load_state_dict(torch.load('models/mytraining5.pt'))\n",
    "print(2)\n",
    "t_model.cuda()\n",
    "t_model.eval()\n",
    "sent = 'david gallo das ist bill lange . ich bin dave gallo .'\n",
    "result = translate_sentence(sent, t_model, input_lang, output_lang, 20)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
