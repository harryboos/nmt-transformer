{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from torch import  optim\n",
    "import numpy as np\n",
    "import copy\n",
    "import argparse\n",
    "from Mask import create_masks\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK_token = 3\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token:\"UNK\"}\n",
    "        self.n_words = 4 # Count SOS and EOS\n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "#     s = s.lower() \n",
    "    s = re.sub(r\"([.!?,])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?,ÄäÖöÜüẞß']+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    pairs = []\n",
    "    line1 = open('data/train.de', encoding='utf-8').read().strip().split('\\n')\n",
    "    line2 = open('data/train.en', encoding='utf-8').read().strip().split('\\n')  #.splitlines()\n",
    "    \n",
    "    for i in range(len(line1)):\n",
    "        \n",
    "        pairs.append([normalize_string(line1[i]), normalize_string(line2[i])])\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 196884 sentence pairs\n",
      "Indexing words...\n",
      "['Nun ist der Feind innerhalb des Walls .', 'Now the enemy is inside the walls .']\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('ger', 'en')\n",
    "\n",
    "# Print an example pair\n",
    "print(random.choice(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Das meiste ist unerforscht , und doch gibt es Schönheiten wie diese , die uns fesseln und uns vertrauter mit ihm machen .', \"It's mostly unexplored , and yet there are beautiful sights like this that captivate us and make us become familiar with it .\"]\n"
     ]
    }
   ],
   "source": [
    "print(pairs[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def variables_from_pair(pair, input_lang, output_lang):\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_len(pair):\n",
    "    result = 0\n",
    "    \n",
    "    \n",
    "    for sents in pair:\n",
    "        for item in sents:\n",
    "            if len(item.split()) > result:\n",
    "                \n",
    "                result = len(item.split())\n",
    "    return result\n",
    "\n",
    "def find_len(element):\n",
    "    result = 0\n",
    "    for item in element:\n",
    "        if len(item.split()) > result:\n",
    "            result = len(item.split())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paddingSOS(vector, max_len):\n",
    "    vector = [SOS_token]+vector\n",
    "    while len(vector)< max_len:\n",
    "        vector.append(PAD_token)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paddingEOS(vector, max_len):\n",
    "    vector = vector + [EOS_token]\n",
    "    while len(vector)< max_len:\n",
    "        vector.append(PAD_token)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_both(vector, max_len):\n",
    "    vector = [SOS_token]+ vector + [EOS_token]\n",
    "    while len(vector)< max_len:\n",
    "        vector.append(PAD_token)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(vector, max_len):\n",
    "    \n",
    "    while len(vector)< max_len:\n",
    "        vector.append(PAD_token)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "input_lang  0  :  PAD\n",
      "input_lang  1  :  SOS\n",
      "input_lang  2  :  EOS\n",
      "input_lang  3  :  UNK\n",
      "input_lang  4  :  David\n",
      "input_lang  5  :  Gallo\n",
      "input_lang  6  :  Das\n",
      "input_lang  7  :  ist\n",
      "output: \n",
      "input_lang  0  :  PAD\n",
      "input_lang  1  :  SOS\n",
      "input_lang  2  :  EOS\n",
      "input_lang  3  :  UNK\n",
      "input_lang  4  :  David\n",
      "input_lang  5  :  Gallo\n",
      "input_lang  6  :  This\n",
      "input_lang  7  :  is\n"
     ]
    }
   ],
   "source": [
    "def print_head(lang):\n",
    "    for i in range(8):\n",
    "        print('input_lang ', i,' : ',lang.index2word[i])\n",
    "print('input: ')\n",
    "print_head(input_lang)\n",
    "print('output: ')\n",
    "print_head(output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len:  621\n"
     ]
    }
   ],
   "source": [
    "max_len = find_max_len(pairs)+2\n",
    "print('max_len: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  ger\n",
      "output:  en\n"
     ]
    }
   ],
   "source": [
    "print('input: ', input_lang.name)\n",
    "print('output: ', output_lang.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_to_indexes(pairs, max_len, input_lang, output_lang):\n",
    "    source = np.zeros((len(pairs), max_len))\n",
    "    target = np.zeros((len(pairs), max_len))\n",
    "    for i in range(len(pairs)):\n",
    "        # add start token for english\n",
    "        sent2 = padding_both(indexes_from_sentence(output_lang, pairs[i][1]), max_len)\n",
    "        sent2 = torch.Tensor(sent2)\n",
    "        \n",
    "        target[i] = sent2\n",
    "        \n",
    "        # add end token for german\n",
    "        sent1 = padding(indexes_from_sentence(input_lang, pairs[i][0]), max_len)\n",
    "        sent1 = torch.Tensor(sent1)\n",
    "        source[i] = sent1\n",
    "    \n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999812072083054\n"
     ]
    }
   ],
   "source": [
    "data_1 = [element for element in pairs if find_len(element) < 200]\n",
    "print(len(data_1) / len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correspond_len(pair, thres):\n",
    "    le = find_len(pair)\n",
    "    if le < thres[0]-3:\n",
    "        return thres[0]\n",
    "    for i in range(len(thres)):\n",
    "        if i == len(thres)-1:\n",
    "            return None\n",
    "        if le > (thres[i]-3) and le < (thres[i+1]-3):\n",
    "            return thres[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_data(data_pairs):\n",
    "    threshold = [20, 40, 60, 80, 100, 200]\n",
    "    class_pairs = []\n",
    "    for i in range(len(threshold)):\n",
    "        class_pairs.append([])\n",
    "    for pair in data_pairs:\n",
    "        pair_len = correspond_len(pair, threshold)\n",
    "        if pair_len is None:\n",
    "            continue\n",
    "        class_pairs[threshold.index(pair_len)].append(pair)\n",
    "        \n",
    "    return class_pairs, threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pairs, thres = class_data(data_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98382\n",
      "71526\n",
      "14677\n",
      "2791\n",
      "619\n",
      "269\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(len(class_pairs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en 58313\n",
      "ger 122633\n"
     ]
    }
   ],
   "source": [
    "print(output_lang.name, output_lang.n_words)\n",
    "print(input_lang.name, input_lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (embed): GermanEmbedder(\n",
       "      (embed): Embedding(122633, 300)\n",
       "    )\n",
       "    (position_encoder): PositionalEncoder()\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (multi_attention): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (feedforward): FeedForward(\n",
       "          (linear_1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (linear_2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        )\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (dropout_1): Dropout(p=0.1)\n",
       "        (dropout_2): Dropout(p=0.1)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (multi_attention): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (feedforward): FeedForward(\n",
       "          (linear_1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (linear_2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        )\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (dropout_1): Dropout(p=0.1)\n",
       "        (dropout_2): Dropout(p=0.1)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (multi_attention): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (feedforward): FeedForward(\n",
       "          (linear_1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (linear_2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        )\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (dropout_1): Dropout(p=0.1)\n",
       "        (dropout_2): Dropout(p=0.1)\n",
       "      )\n",
       "      (3): EncoderLayer(\n",
       "        (multi_attention): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (feedforward): FeedForward(\n",
       "          (linear_1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (linear_2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        )\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (dropout_1): Dropout(p=0.1)\n",
       "        (dropout_2): Dropout(p=0.1)\n",
       "      )\n",
       "      (4): EncoderLayer(\n",
       "        (multi_attention): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (feedforward): FeedForward(\n",
       "          (linear_1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (linear_2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        )\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (dropout_1): Dropout(p=0.1)\n",
       "        (dropout_2): Dropout(p=0.1)\n",
       "      )\n",
       "      (5): EncoderLayer(\n",
       "        (multi_attention): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (feedforward): FeedForward(\n",
       "          (linear_1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (linear_2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        )\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (dropout_1): Dropout(p=0.1)\n",
       "        (dropout_2): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embed): EnglishEmbedder(\n",
       "      (embed): Embedding(58313, 300)\n",
       "    )\n",
       "    (position_encoder): PositionalEncoder()\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.1)\n",
       "        (dropout_2): Dropout(p=0.1)\n",
       "        (dropout_3): Dropout(p=0.1)\n",
       "        (multi_attention1): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (multi_attention2): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (linear_2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.1)\n",
       "        (dropout_2): Dropout(p=0.1)\n",
       "        (dropout_3): Dropout(p=0.1)\n",
       "        (multi_attention1): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (multi_attention2): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (linear_2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.1)\n",
       "        (dropout_2): Dropout(p=0.1)\n",
       "        (dropout_3): Dropout(p=0.1)\n",
       "        (multi_attention1): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (multi_attention2): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (linear_2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.1)\n",
       "        (dropout_2): Dropout(p=0.1)\n",
       "        (dropout_3): Dropout(p=0.1)\n",
       "        (multi_attention1): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (multi_attention2): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (linear_2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.1)\n",
       "        (dropout_2): Dropout(p=0.1)\n",
       "        (dropout_3): Dropout(p=0.1)\n",
       "        (multi_attention1): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (multi_attention2): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (linear_2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (norm_1): Norm()\n",
       "        (norm_2): Norm()\n",
       "        (norm_3): Norm()\n",
       "        (dropout_1): Dropout(p=0.1)\n",
       "        (dropout_2): Dropout(p=0.1)\n",
       "        (dropout_3): Dropout(p=0.1)\n",
       "        (multi_attention1): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (multi_attention2): MultiHeadAttention(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (q_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (k_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (v_Linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "          (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "        (ff): FeedForward(\n",
       "          (linear_1): Linear(in_features=300, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (linear_2): Linear(in_features=2048, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=300, out_features=58313, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Model import Transformer\n",
    "params = {}\n",
    "\n",
    "\n",
    "params['batch_size'] = 64\n",
    "params['epochs'] = 50\n",
    "params['learning_rate'] = 0.001\n",
    "\n",
    "\n",
    "dim_model = 300\n",
    "H = 12\n",
    "N = 6\n",
    "src_vocab = input_lang.n_words\n",
    "trg_vocab = output_lang.n_words\n",
    "\n",
    "model = Transformer(src_vocab, trg_vocab, dim_model, N, H)\n",
    "model.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: 1. add bleu score for each epoch 2. change datas to pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# dataset: pairs\n",
    "def train_lm(data_pairs, params, net):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "    \n",
    "    \n",
    "    classed_pairs, thres = class_data(data_pairs)\n",
    "    sources_set = []\n",
    "    targets_set = []\n",
    "    batches_set = []\n",
    "    for i in range(len(classed_pairs)):\n",
    "        source, target = pair_to_indexes(classed_pairs[i], thres[i], input_lang, output_lang)\n",
    "        sources = torch.from_numpy(source)\n",
    "        targets = torch.from_numpy(target)\n",
    "        sources = sources.cuda()\n",
    "        targets = targets.cuda()\n",
    "    \n",
    "        num_examples = len(classed_pairs[i])\n",
    "        batches = [(start, start + params['batch_size']) for start in \\\n",
    "               range(0, num_examples, params['batch_size'])]\n",
    "        sources_set.append(sources)\n",
    "        targets_set.append(targets)\n",
    "        batches_set.append(batches)\n",
    "    file = open('models/loss.txt','w') \n",
    "    for epoch in range(params['epochs']):\n",
    "        ep_loss = 0.\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # for each batch, calculate loss and optimize model parameters\n",
    "        for i in range(len(batches_set)):\n",
    "            batches = batches_set[i]\n",
    "            random.shuffle(batches)\n",
    "            sources = sources_set[i]\n",
    "            targets = targets_set[i]\n",
    "            for b_idx, (start, end) in enumerate(batches):\n",
    "                source = sources[start:end]\n",
    "                target = targets[start:end]\n",
    "\n",
    "                source_mask, target_mask = create_masks(source.cpu(), target.cpu())\n",
    "                source_mask = source_mask.cuda()\n",
    "                target_mask = target_mask.cuda()\n",
    "                preds = net(source, target, source_mask, target_mask)\n",
    "\n",
    "\n",
    "                preds = preds[:, :-1, :].contiguous().view(-1, net.target_vocab)\n",
    "                labels = target[:, 1:].contiguous().view(-1)\n",
    "                loss = criterion(preds, labels.long())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                ep_loss += loss.item()\n",
    "\n",
    "        curr_loss = 'epoch: %d, loss: %0.2f, time: %0.2f sec' % (epoch, ep_loss, time.time() - start_time)\n",
    "        print(curr_loss)\n",
    "        substr = 'mytraining'+str(epoch)+'.pt'\n",
    "        path = 'models/'  +substr\n",
    "        torch.save(net.state_dict(), path)\n",
    "        \n",
    "        file.write(curr_loss+'\\n')\n",
    "\n",
    "        \n",
    "    file.close()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 63.35, time: 3.43 sec\n",
      "epoch: 1, loss: 86.48, time: 0.98 sec\n",
      "epoch: 2, loss: 81.25, time: 0.94 sec\n",
      "epoch: 3, loss: 59.29, time: 0.98 sec\n",
      "epoch: 4, loss: 42.36, time: 1.01 sec\n",
      "epoch: 5, loss: 38.31, time: 0.98 sec\n",
      "epoch: 6, loss: 39.10, time: 0.94 sec\n",
      "epoch: 7, loss: 45.53, time: 1.06 sec\n",
      "epoch: 8, loss: 33.24, time: 0.93 sec\n",
      "epoch: 9, loss: 24.49, time: 0.97 sec\n",
      "epoch: 10, loss: 17.64, time: 0.98 sec\n",
      "epoch: 11, loss: 22.72, time: 1.00 sec\n",
      "epoch: 12, loss: 21.72, time: 0.98 sec\n",
      "epoch: 13, loss: 17.38, time: 0.95 sec\n",
      "epoch: 14, loss: 15.09, time: 0.96 sec\n",
      "epoch: 15, loss: 15.15, time: 0.97 sec\n",
      "epoch: 16, loss: 18.45, time: 0.97 sec\n",
      "epoch: 17, loss: 15.35, time: 1.05 sec\n",
      "epoch: 18, loss: 13.10, time: 1.41 sec\n",
      "epoch: 19, loss: 9.80, time: 0.97 sec\n",
      "epoch: 20, loss: 8.94, time: 1.02 sec\n",
      "epoch: 21, loss: 7.91, time: 1.03 sec\n",
      "epoch: 22, loss: 6.72, time: 0.97 sec\n",
      "epoch: 23, loss: 6.07, time: 0.95 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-6ae0a855cb05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_lm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-55bb881b7d2c>\u001b[0m in \u001b[0;36mtrain_lm\u001b[1;34m(data_pairs, params, net)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0msubstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'mytraining'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.pt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'models/'\u001b[0m  \u001b[1;33m+\u001b[0m\u001b[0msubstr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurr_loss\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\huzih\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \"\"\"\n\u001b[1;32m--> 219\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\huzih\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[1;34m(f, mode, body)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\huzih\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m     \"\"\"\n\u001b[1;32m--> 219\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\huzih\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mserialized_storage_keys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[0mserialized_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_should_read_directly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_lm(data_1, params, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## restore model from .pt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traslante_sentence(curr_sent, max_len_1, input_lang, output_lang):\n",
    "    source, target = pair_to_indexes(curr_sent, max_len_1, input_lang, output_lang)\n",
    "    target_fake = np.zeros((1, max_len_1))\n",
    "    target_fake[0][0] = 1\n",
    "    target_temp = target_fake\n",
    "    \n",
    "    for i in range(max_len_1-2):\n",
    "        sou = torch.from_numpy(source)\n",
    "        tar = torch.from_numpy(target_fake)\n",
    "        sou = sou.cuda()\n",
    "        tar = tar.cuda()\n",
    "        source_mask, target_mask = create_masks(sou.cpu(), tar.cpu())\n",
    "        source_mask = source_mask.cuda()\n",
    "        target_mask = target_mask.cuda()\n",
    "        preds = model(sou, tar, source_mask, target_mask)\n",
    "\n",
    "        preds = preds[:, :-1,:].contiguous().view(-1, model.target_vocab)\n",
    "        ss = torch.softmax(preds, dim=-1)\n",
    "        mm = torch.max(ss, dim=-1)[1]\n",
    "        target_temp[0][i+1] = mm[i]\n",
    "        target_fake = target_temp\n",
    "    result = ''\n",
    "    for idx in mm:\n",
    "        if idx == 0:\n",
    "            break\n",
    "        index = idx.item()\n",
    "        if index == 2:\n",
    "            break\n",
    "        result += output_lang.index2word[index]+' '\n",
    "    print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
