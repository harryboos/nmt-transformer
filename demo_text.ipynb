{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from torch import  optim\n",
    "import numpy as np\n",
    "import copy\n",
    "import argparse\n",
    "from Mask import create_masks\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK_token = 3\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\", UNK_token:\"UNK\"}\n",
    "        self.n_words = 4 # Count SOS and EOS\n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "#     s = s.lower() \n",
    "    s = re.sub(r\"([.!?,])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?,ÄäÖöÜüẞß']+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    pairs = []\n",
    "    line1 = open('data/train.de', encoding='utf-8').read().strip().split('\\n')\n",
    "    line2 = open('data/train.en', encoding='utf-8').read().strip().split('\\n')  #.splitlines()\n",
    "    \n",
    "    for i in range(len(line1)):\n",
    "        \n",
    "        pairs.append([normalize_string(line1[i]), normalize_string(line2[i])])\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 196884 sentence pairs\n",
      "Indexing words...\n",
      "['Hier ist etwas Interessantes für die IT Welt Bio Silikon . Das sind Kieselalgen , die aus Silikaten entstehen .', \"Here's an interesting one for the IT world bio silicon . This is a diatom , which is made of silicates .\"]\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('ger', 'en')\n",
    "\n",
    "# Print an example pair\n",
    "print(random.choice(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Das meiste ist unerforscht , und doch gibt es Schönheiten wie diese , die uns fesseln und uns vertrauter mit ihm machen .', \"It's mostly unexplored , and yet there are beautiful sights like this that captivate us and make us become familiar with it .\"]\n"
     ]
    }
   ],
   "source": [
    "print(pairs[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def variables_from_pair(pair, input_lang, output_lang):\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_len(pair):\n",
    "    result = 0\n",
    "    \n",
    "    \n",
    "    for sents in pair:\n",
    "        for item in sents:\n",
    "            if len(item.split()) > result:\n",
    "                \n",
    "                result = len(item.split())\n",
    "    return result\n",
    "\n",
    "def find_len(element):\n",
    "    result = 0\n",
    "    for item in element:\n",
    "        if len(item.split()) > result:\n",
    "            result = len(item.split())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paddingSOS(vector, max_len):\n",
    "    vector = [SOS_token]+vector\n",
    "    while len(vector)< max_len:\n",
    "        vector.append(PAD_token)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paddingEOS(vector, max_len):\n",
    "    vector = vector + [EOS_token]\n",
    "    while len(vector)< max_len:\n",
    "        vector.append(PAD_token)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_both(vector, max_len):\n",
    "    vector = [SOS_token]+ vector + [EOS_token]\n",
    "    while len(vector)< max_len:\n",
    "        vector.append(PAD_token)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(vector, max_len):\n",
    "    \n",
    "    while len(vector)< max_len:\n",
    "        vector.append(PAD_token)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      "input_lang  0  :  PAD\n",
      "input_lang  1  :  SOS\n",
      "input_lang  2  :  EOS\n",
      "input_lang  3  :  UNK\n",
      "input_lang  4  :  David\n",
      "input_lang  5  :  Gallo\n",
      "input_lang  6  :  Das\n",
      "input_lang  7  :  ist\n",
      "output: \n",
      "input_lang  0  :  PAD\n",
      "input_lang  1  :  SOS\n",
      "input_lang  2  :  EOS\n",
      "input_lang  3  :  UNK\n",
      "input_lang  4  :  David\n",
      "input_lang  5  :  Gallo\n",
      "input_lang  6  :  This\n",
      "input_lang  7  :  is\n"
     ]
    }
   ],
   "source": [
    "def print_head(lang):\n",
    "    for i in range(8):\n",
    "        print('input_lang ', i,' : ',lang.index2word[i])\n",
    "print('input: ')\n",
    "print_head(input_lang)\n",
    "print('output: ')\n",
    "print_head(output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len:  621\n"
     ]
    }
   ],
   "source": [
    "max_len = find_max_len(pairs)+2\n",
    "print('max_len: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  ger\n",
      "output:  en\n"
     ]
    }
   ],
   "source": [
    "print('input: ', input_lang.name)\n",
    "print('output: ', output_lang.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_to_indexes(pairs, max_len, input_lang, output_lang):\n",
    "    source = np.zeros((len(pairs), max_len))\n",
    "    target = np.zeros((len(pairs), max_len))\n",
    "    for i in range(len(pairs)):\n",
    "        # add start token for english\n",
    "        sent2 = padding_both(indexes_from_sentence(output_lang, pairs[i][1]), max_len)\n",
    "        sent2 = torch.Tensor(sent2)\n",
    "        \n",
    "        target[i] = sent2\n",
    "        \n",
    "        # add end token for german\n",
    "        sent1 = padding(indexes_from_sentence(input_lang, pairs[i][0]), max_len)\n",
    "        sent1 = torch.Tensor(sent1)\n",
    "        source[i] = sent1\n",
    "    \n",
    "    return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999812072083054\n"
     ]
    }
   ],
   "source": [
    "data_1 = [element for element in pairs if find_len(element) < 200]\n",
    "print(len(data_1) / len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correspond_len(pair, thres):\n",
    "    le = find_len(pair)\n",
    "    if le < thres[0]-3:\n",
    "        return thres[0]\n",
    "    for i in range(len(thres)):\n",
    "        if i == len(thres)-1:\n",
    "            return None\n",
    "        if le > (thres[i]-3) and le < (thres[i+1]-3):\n",
    "            return thres[i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_data(data_pairs):\n",
    "    threshold = [20, 40, 60, 80, 100, 200]\n",
    "    class_pairs = []\n",
    "    for i in range(len(threshold)):\n",
    "        class_pairs.append([])\n",
    "    for pair in data_pairs:\n",
    "        pair_len = correspond_len(pair, threshold)\n",
    "        if pair_len is None:\n",
    "            continue\n",
    "        class_pairs[threshold.index(pair_len)].append(pair)\n",
    "        \n",
    "    return class_pairs, threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_pairs, thres = class_data(data_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98382\n",
      "71526\n",
      "14677\n",
      "2791\n",
      "619\n",
      "269\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(len(class_pairs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en 58313\n",
      "ger 122633\n"
     ]
    }
   ],
   "source": [
    "print(output_lang.name, output_lang.n_words)\n",
    "print(input_lang.name, input_lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import Transformer\n",
    "params = {}\n",
    "\n",
    "\n",
    "params['batch_size'] = 2\n",
    "params['epochs'] = 30\n",
    "params['learning_rate'] = 0.001\n",
    "\n",
    "\n",
    "dim_model = 300\n",
    "H = 12\n",
    "N = 6\n",
    "src_vocab = input_lang.n_words\n",
    "trg_vocab = output_lang.n_words\n",
    "\n",
    "model = Transformer(src_vocab, trg_vocab, dim_model, N, H)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: 1. add bleu score for each epoch 2. change datas to pairs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# dataset: pairs\n",
    "def train_lm(data_pairs, params, net):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params['learning_rate'])\n",
    "    \n",
    "    \n",
    "    classed_pairs, thres = class_data(data_pairs)\n",
    "    sources_set = []\n",
    "    targets_set = []\n",
    "    batches_set = []\n",
    "    for i in range(len(classed_pairs)):\n",
    "        source, target = pair_to_indexes(classed_pairs[i], thres[i], input_lang, output_lang)\n",
    "        sources = torch.from_numpy(source)\n",
    "        targets = torch.from_numpy(target)\n",
    "    \n",
    "        num_examples = len(classed_pairs[i])\n",
    "        batches = [(start, start + params['batch_size']) for start in \\\n",
    "               range(0, num_examples, params['batch_size'])]\n",
    "        sources_set.append(sources)\n",
    "        targets_set.append(targets)\n",
    "        batches_set.append(batches)\n",
    "\n",
    "    for epoch in range(params['epochs']):\n",
    "        ep_loss = 0.\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # for each batch, calculate loss and optimize model parameters\n",
    "        for i in range(len(batches_set)):\n",
    "            batches = batches_set[i]\n",
    "            random.shuffle(batches)\n",
    "            sources = sources_set[i]\n",
    "            targets = targets_set[i]\n",
    "            for b_idx, (start, end) in enumerate(batches):\n",
    "                source = sources[start:end]\n",
    "                target = targets[start:end]\n",
    "\n",
    "                source_mask, target_mask = create_masks(source, target)\n",
    "                preds = net(source, target, source_mask, target_mask)\n",
    "\n",
    "\n",
    "                preds = preds[:, :-1, :].contiguous().view(-1, net.target_vocab)\n",
    "                labels = target[:, 1:].contiguous().view(-1)\n",
    "                loss = criterion(preds, labels.long())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                ep_loss += loss.item()\n",
    "\n",
    "        curr_loss = 'epoch: %d, loss: %0.2f, time: %0.2f sec' % (epoch, ep_loss, time.time() - start_time)\n",
    "        print(curr_loss)\n",
    "\n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " #[20, 40, 60, 80, 100, 200]\n",
    "# for i in range(len(data_1)):\n",
    "#     if find_len(data_1[i]) > 600:\n",
    "#         print('100', i)\n",
    "#     elif find_len(data_1[i]) > 80:\n",
    "#         print('80', i)\n",
    "#     elif find_len(data_1[i]) > 60:\n",
    "#         print('60',i)\n",
    "#     elif find_len(data_1[i]) > 40:\n",
    "#         print('40',i)\n",
    "#     elif find_len(data_1[i]) > 20:\n",
    "#         print('20',i)\n",
    "    \n",
    "# train_lm(data_1, params, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp = data_1[:5]\n",
    "data_temp.append(data_1[14])\n",
    "data_temp.append(data_1[16])\n",
    "data_temp.append(data_1[30])\n",
    "data_temp.append(data_1[36])\n",
    "data_temp.append(data_1[1240])\n",
    "data_temp.append(data_1[1478])\n",
    "data_temp.append(data_1[56354])\n",
    "data_temp.append(data_1[55323])\n",
    "data_temp.append(data_1[57843])\n",
    "data_temp.append(data_1[58482])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 180.72, time: 10.61 sec\n",
      "epoch: 1, loss: 166.62, time: 10.03 sec\n",
      "epoch: 2, loss: 99.96, time: 9.98 sec\n",
      "epoch: 3, loss: 77.18, time: 10.03 sec\n",
      "epoch: 4, loss: 65.42, time: 10.12 sec\n",
      "epoch: 5, loss: 55.51, time: 9.88 sec\n",
      "epoch: 6, loss: 55.32, time: 10.25 sec\n",
      "epoch: 7, loss: 54.64, time: 10.04 sec\n",
      "epoch: 8, loss: 44.67, time: 9.97 sec\n",
      "epoch: 9, loss: 40.43, time: 9.96 sec\n",
      "epoch: 10, loss: 38.98, time: 10.06 sec\n",
      "epoch: 11, loss: 40.85, time: 10.21 sec\n",
      "epoch: 12, loss: 40.37, time: 10.06 sec\n",
      "epoch: 13, loss: 37.21, time: 10.02 sec\n",
      "epoch: 14, loss: 38.52, time: 9.94 sec\n",
      "epoch: 15, loss: 39.24, time: 10.12 sec\n",
      "epoch: 16, loss: 33.70, time: 10.45 sec\n",
      "epoch: 17, loss: 32.74, time: 10.91 sec\n",
      "epoch: 18, loss: 32.85, time: 10.13 sec\n",
      "epoch: 19, loss: 30.18, time: 10.09 sec\n",
      "epoch: 20, loss: 29.53, time: 10.07 sec\n",
      "epoch: 21, loss: 31.22, time: 10.07 sec\n",
      "epoch: 22, loss: 28.62, time: 10.78 sec\n",
      "epoch: 23, loss: 24.38, time: 10.07 sec\n",
      "epoch: 24, loss: 20.82, time: 10.07 sec\n",
      "epoch: 25, loss: 20.69, time: 10.02 sec\n",
      "epoch: 26, loss: 18.91, time: 10.03 sec\n",
      "epoch: 27, loss: 20.86, time: 10.14 sec\n",
      "epoch: 28, loss: 15.50, time: 10.05 sec\n",
      "epoch: 29, loss: 13.85, time: 10.09 sec\n"
     ]
    }
   ],
   "source": [
    "train_lm(data_temp, params, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 24.12, time: 10.53 sec\n",
      "epoch: 1, loss: 19.99, time: 10.15 sec\n",
      "epoch: 2, loss: 21.23, time: 10.58 sec\n",
      "epoch: 3, loss: 22.57, time: 10.24 sec\n",
      "epoch: 4, loss: 17.16, time: 10.08 sec\n",
      "epoch: 5, loss: 15.31, time: 10.00 sec\n",
      "epoch: 6, loss: 14.73, time: 10.02 sec\n",
      "epoch: 7, loss: 11.65, time: 10.05 sec\n",
      "epoch: 8, loss: 10.22, time: 10.12 sec\n",
      "epoch: 9, loss: 9.12, time: 10.17 sec\n",
      "epoch: 10, loss: 8.33, time: 10.19 sec\n",
      "epoch: 11, loss: 7.62, time: 10.16 sec\n",
      "epoch: 12, loss: 7.69, time: 10.09 sec\n",
      "epoch: 13, loss: 7.31, time: 10.00 sec\n",
      "epoch: 14, loss: 6.11, time: 10.09 sec\n",
      "epoch: 15, loss: 5.75, time: 10.15 sec\n",
      "epoch: 16, loss: 6.23, time: 10.01 sec\n",
      "epoch: 17, loss: 5.11, time: 10.09 sec\n",
      "epoch: 18, loss: 3.84, time: 10.05 sec\n",
      "epoch: 19, loss: 3.40, time: 10.05 sec\n",
      "epoch: 20, loss: 3.33, time: 10.15 sec\n",
      "epoch: 21, loss: 2.49, time: 10.32 sec\n",
      "epoch: 22, loss: 1.83, time: 10.59 sec\n",
      "epoch: 23, loss: 2.29, time: 10.09 sec\n",
      "epoch: 24, loss: 2.08, time: 10.25 sec\n",
      "epoch: 25, loss: 1.67, time: 10.06 sec\n",
      "epoch: 26, loss: 2.30, time: 10.07 sec\n",
      "epoch: 27, loss: 1.80, time: 10.09 sec\n",
      "epoch: 28, loss: 1.20, time: 9.99 sec\n",
      "epoch: 29, loss: 0.69, time: 10.05 sec\n"
     ]
    }
   ],
   "source": [
    "train_lm(data_temp, params, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## restore model from .pt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traslante_sentence(curr_sent, max_len_1, input_lang, output_lang):\n",
    "    source, target = pair_to_indexes(curr_sent, max_len_1, input_lang, output_lang)\n",
    "    target_fake = np.zeros((1, max_len_1))\n",
    "    target_fake[0][0] = 1\n",
    "    target_temp = target_fake\n",
    "    \n",
    "    for i in range(max_len_1-2):\n",
    "        sou = torch.from_numpy(source)\n",
    "        tar = torch.from_numpy(target_fake)\n",
    "        source_mask, target_mask = create_masks(sou, tar)\n",
    "        preds = model(sou, tar, source_mask, target_mask)\n",
    "\n",
    "        preds = preds[:, :-1,:].contiguous().view(-1, model.target_vocab)\n",
    "        ss = torch.softmax(preds, dim=-1)\n",
    "        mm = torch.max(ss, dim=-1)[1]\n",
    "        target_temp[0][i+1] = mm[i]\n",
    "        target_fake = target_temp\n",
    "    result = ''\n",
    "    for idx in mm:\n",
    "        if idx == 0:\n",
    "            break\n",
    "        index = idx.item()\n",
    "        if index == 2:\n",
    "            break\n",
    "        result += output_lang.index2word[index]+' '\n",
    "    print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Wir haben ein paar der unglaublichsten Aufnahmen der Titanic , die man je gesehen hat , , und wir werden Ihnen nichts davon zeigen .', \"We've got some of the most incredible video of Titanic that's ever been seen , and we're not going to show you any of it .\"]]\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "data_curr = pairs[2:3]\n",
    "print(data_curr)\n",
    "print(find_len(data_curr[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've got some of the most incredible video of Titanic that's ever been seen , and we're not going to show you any of it . \n"
     ]
    }
   ],
   "source": [
    "traslante_sentence(data_curr, 40, input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bcolz\n",
    "# import pickle\n",
    "# vectors = bcolz.open(f'emb/6B.300.dat')[:]\n",
    "# words = pickle.load(open(f'emb/6B.300_words.pkl', 'rb'))\n",
    "# word2idx = pickle.load(open(f'emb/6B.300_idx.pkl', 'rb'))\n",
    "\n",
    "# eng = {w: vectors[word2idx[w]] for w in words}\n",
    "\n",
    "# vectors = bcolz.open(f'emb/ger.300.dat')[:]\n",
    "# words = pickle.load(open(f'emb/ger.300_words.pkl', 'rb'))\n",
    "# word2idx = pickle.load(open(f'emb/ger.300_idx.pkl', 'rb'))\n",
    "\n",
    "# ger = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(eng['hello']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output_lang.name, output_lang.n_words)\n",
    "# print(input_lang.name, input_lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# input_emb = np.zeros((input_lang.n_words, 300))\n",
    "# for i in range(input_lang.n_words):\n",
    "#     curr_word = input_lang.index2word[i]\n",
    "#     try:\n",
    "#         vec = ger[curr_word.lower()]\n",
    "#         input_emb[i] = vec\n",
    "#     except KeyError:\n",
    "#         vec = torch.rand((1, 300))\n",
    "#         input_emb[i] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # save\n",
    "# np.save('emb/ger', input_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.160972  0.006209 -0.008173 -0.484031  0.067782  0.242032  0.437631\n",
      " -0.121384 -0.247417  0.112258]\n"
     ]
    }
   ],
   "source": [
    "# input_reload = np.load('emb/ger.npy')\n",
    "# print(input_reload[20][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_emb = np.zeros((output_lang.n_words, 300))\n",
    "# for i in range(output_lang.n_words):\n",
    "#     curr_word = output_lang.index2word[i]\n",
    "#     try:\n",
    "#         vec = eng[curr_word.lower()]\n",
    "#         output_emb[i] = vec\n",
    "#     except KeyError:\n",
    "#         vec = torch.rand((1, 300))\n",
    "#         output_emb[i] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save\n",
    "# np.save('emb/eng', output_emb)\n",
    "# output_reload = np.load('emb/eng.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.25831   0.43644  -0.1138   -0.5259    0.20213   0.95247  -0.58764\n",
      " -0.047001 -0.053704 -1.744   ]\n",
      "141\n",
      "[-0.25831   0.43644  -0.1138   -0.5259    0.20213   0.95247  -0.58764\n",
      " -0.047001 -0.053704 -1.744   ]\n",
      "[-0.25831   0.43644  -0.1138   -0.5259    0.20213   0.95247  -0.58764\n",
      " -0.047001 -0.053704 -1.744   ]\n"
     ]
    }
   ],
   "source": [
    "# print(eng['world'][:10])\n",
    "# print(output_lang.word2index['world'])\n",
    "# print(output_emb[141][:10])\n",
    "# print(output_reload[141][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv36",
   "language": "python",
   "name": "myenv36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
